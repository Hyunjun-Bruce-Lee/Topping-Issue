{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyOpenSSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium option\n",
    "option = Options()\n",
    "option.add_argument('--headless') # 헤드레스\n",
    "option.add_argument('--window-size=1890,1030') # 창 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumwire import webdriver\n",
    "\n",
    "# chromedriver\n",
    "driver = webdriver.Chrome(\"C:/python/chromedriver_win32_93/chromedriver.exe\", options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.date_range(start='20210223', end='20210430')\n",
    "date_list = date_index.strftime(\"%Y%m%d\").tolist()\n",
    "authorization = 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJzY29wZSI6W10sImV4cCI6MTYzMjc2MzQ1OSwiYXV0aG9yaXRpZXMiOlsiUk9MRV9DTElFTlQiXSwianRpIjoiZTk3YjlmODktNGM5MS00YzFhLWE5ZjMtMGYxY2E3ZjNlZTA2IiwiZm9ydW1faWQiOi05OSwiY2xpZW50X2lkIjoiMjZCWEF2S255NVdGNVowOWxyNWs3N1k4In0.31I1g2fheU6zb0T2hlX4xcOjfzRb26pOLjT4kqopZ0Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category = ['politics', 'economic', 'society',  'culture', 'foreign', 'digital']\n",
    "id_list = ['10000', '10100', '10200', '10300', '10400', '10500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# get news content\n",
    "def get_detail(url):\n",
    "    body = []\n",
    "    punc = '[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~“”·]'\n",
    "    reg = re.compile('[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+[a-zA-Z0-9-.]+$')\n",
    "    response = requests.get(url)\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    for p in root.xpath('//*[@id=\"harmonyContainer\"]/section/p'):\n",
    "        if p.text: # 체크\n",
    "            temp = re.sub(punc, '', p.text)\n",
    "            temp = re.sub(reg, '', temp)\n",
    "            body.append(temp) # 특수문자 제거 / 메일주소 제거\n",
    "    full_body = ' '.join(body)\n",
    "    \n",
    "    return full_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get news comment\n",
    "def get_comment(news_id, authorization):\n",
    "    list_comment = []\n",
    "    url = 'https://comment.daum.net/apis/v1/posts/@{}/comments?'.format(news_id)\n",
    "    params = {'parentId' : '0', 'offset' : '0', 'limit' : '100', 'sort' : 'RECOMMEND', 'isInitial' : 'true'}\n",
    "    headers = {'Authorization' : authorization\n",
    "              }\n",
    "    response = requests.get(url, headers = headers, params = params)\n",
    "    status_code = response.status_code\n",
    "    comment_all = response.json()\n",
    "\n",
    "    for i in comment_all:\n",
    "        li = []\n",
    "        li.append(i['content'])\n",
    "        li.append(float(i['likeCount']))\n",
    "        li.append(float(i['dislikeCount']))\n",
    "        list_comment.append(li)\n",
    "    \n",
    "    if len(list_comment) == 0:\n",
    "        list_comment.append('NA')\n",
    "\n",
    "    return list_comment    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comment count\n",
    "def get_comment_cnt(news_id, authorization):\n",
    "    url = \"https://comment.daum.net/apis/v1/ui/single/main/@{}\".format(news_id)\n",
    "    header = {\n",
    "        \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "        \"referer\": url,\n",
    "        'Authorization' : authorization\n",
    "    }\n",
    "    raw = requests.get(url, headers=header)\n",
    "\n",
    "    s_jsonData = json.loads(raw.text)\n",
    "\n",
    "    cnt = s_jsonData['post']['commentCount']\n",
    "    comment_cnt = float(cnt)\n",
    "    \n",
    "    return comment_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get news sentiment\n",
    "def get_sentiment(news_id, authorization):\n",
    "    \n",
    "    url = 'https://action.daum.net/apis/v1/reactions/home?itemKey={}'.format(news_id)\n",
    "    header = {\n",
    "        \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "        \"referer\": url,\n",
    "        'Authorization' : authorization\n",
    "    }\n",
    "    raw = requests.get(url, headers=header)\n",
    "\n",
    "    s_jsonData = json.loads(raw.text)\n",
    "\n",
    "    sentiment = {\"좋아요\" : 0, \"감동이에요\" : 0, \"슬퍼요\" : 0, \"화나요\" : 0, \"추천해요\" : 0}\n",
    "\n",
    "    sentiment['좋아요'] = s_jsonData['item']['stats']['LIKE']\n",
    "    sentiment['감동이에요'] = s_jsonData['item']['stats']['IMPRESS']\n",
    "    sentiment['슬퍼요'] = s_jsonData['item']['stats']['SAD']\n",
    "    sentiment['화나요'] = s_jsonData['item']['stats']['ANGRY']\n",
    "    sentiment['추천해요'] = s_jsonData['item']['stats']['RECOMMEND']\n",
    "    \n",
    "    #print(sentiment)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "df_list = []\n",
    "id_count = 0\n",
    "for REG_DATE in date_list:\n",
    "    page = 1\n",
    "    max_page = 0\n",
    "    while(True):\n",
    "        df_temp = []\n",
    "        id_temp = 1\n",
    "        # ====== category check ======\n",
    "        # get url\n",
    "        response = requests.get('http://news.daum.net/breakingnews/{}?page={}&regDate={}'\\\n",
    "                                .format(category[2], page, REG_DATE))\n",
    "        root = lxml.html.fromstring(response.content)\n",
    "        for li in root.xpath('//*[@id=\"mArticle\"]/div[3]/ul/li'):\n",
    "            id_num = id_count + 15 * (page - 1) + id_temp\n",
    "            if id_num % 300 == 0:\n",
    "                time.sleep(random.randrange(20,50))\n",
    "                # ====== page check ======\n",
    "                print(id_num)\n",
    "                \n",
    "                \n",
    "            id_temp = id_temp + 1\n",
    "            id_str = str(id_num)\n",
    "            # ====== category check ======\n",
    "            id_fin = id_list[2]+\"_\"+id_str\n",
    "            # get news date\n",
    "            date = datetime.datetime.strptime(REG_DATE, '%Y%m%d')\n",
    "            # get news title\n",
    "            a = li.xpath('div/strong/a')[0]\n",
    "            # get news link\n",
    "            url = a.get('href')\n",
    "            # get news content\n",
    "            content = get_detail(url)\n",
    "            content_len = float(len(content))\n",
    "            \n",
    "            try:\n",
    "                # get news comment\n",
    "                news_id = url.split(\"/\")[-1]\n",
    "                comment = get_comment(news_id, authorization)\n",
    "                if comment[0] == 'NA':\n",
    "                    comment = \"NA\"\n",
    "                comment_cnt = get_comment_cnt(news_id, authorization)\n",
    "                # get news sentiment\n",
    "                sentiment = get_sentiment(news_id, authorization)\n",
    "                \n",
    "            except:\n",
    "                print(\"authorization 에러\")\n",
    "                driver.get(url)\n",
    "                time.sleep(20)\n",
    "                print(\"기존 authorization : \", authorization)\n",
    "                authorization = ''\n",
    "                # get request headers authorization\n",
    "                for request in driver.requests:\n",
    "                    if type(request.headers['authorization']) is str:\n",
    "                        authorization = request.headers['authorization']\n",
    "                        break\n",
    "\n",
    "                news_id = url.split(\"/\")[-1]\n",
    "                print(news_id)\n",
    "                print(\"현재 authorization : \", authorization)\n",
    "                # get news comment\n",
    "                comment = get_comment(news_id, authorization)\n",
    "                if comment[0] == 'NA':\n",
    "                    comment = \"NA\"\n",
    "                comment_cnt = get_comment_cnt(news_id, authorization)\n",
    "                # get news sentiment\n",
    "                sentiment = get_sentiment(news_id, authorization)\n",
    "            \n",
    "            # get press name\n",
    "            try:\n",
    "                press = li.xpath('div/strong/span')[0].text\n",
    "            except:\n",
    "                press = \"NA\"\n",
    "            \n",
    "            # one page news list data to dataframe\n",
    "            df = pd.DataFrame({'ID' : [id_fin], 'Date' : [date], 'Title':[a.text], 'Content' : [content], 'Content_len' : [content_len], \n",
    "                               'Comment' : [comment], 'Comment_cnt' : [comment_cnt], 'Sentiment' : [sentiment],\n",
    "                               'Link' : [url], 'Press' : [press]})\n",
    "            df_temp.append(df)   \n",
    "\n",
    "        if df_temp:\n",
    "            for i in range(len(df_temp)):\n",
    "                df_list.append(df_temp[i])\n",
    "\n",
    "\n",
    "        # get max page number    \n",
    "        for a in root.xpath('//*[@id=\"mArticle\"]/div[3]/div/span/a'):\n",
    "            try:\n",
    "                num = int(a.text)\n",
    "                if max_page < num:\n",
    "                    max_page = num       \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # check last page     \n",
    "        span = root.xpath('//*[@id=\"mArticle\"]/div[3]/div/span/a[@class=\"btn_page btn_next\"]')\n",
    "        \n",
    "        if (len(span) <= 0) & (page >= max_page):\n",
    "            break\n",
    "        else:\n",
    "            page = page + 1\n",
    "        \n",
    "    \n",
    "    id_count = len(df_list)\n",
    "    # ====== date check ======\n",
    "    print(\"======\"+REG_DATE+\"=========\")\n",
    "\n",
    "df_list_all = pd.concat(df_list)\n",
    "df_list_all = df_list_all.reset_index(drop = True)\n",
    "\n",
    "# save binary file using pickle\n",
    "# ====== category check ======\n",
    "with open('result/'+id_list[2]+'_2104.bin', 'wb') as f:\n",
    "     pickle.dump(df_list_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load binary file using pickle\n",
    "with open('result/10200_2104.bin', 'rb') as f:\n",
    "     load_data = pickle.load(f)\n",
    "load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
